%Template
% !TeX spellcheck = de 
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{geometry,forloop,fancyhdr,fancybox,lastpage}
\usepackage{listings}
\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=left,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
% Diese Daten müssen pro Blatt angepasst werden:
\newcommand{\NUMBER}{6}
\newcommand{\EXERCISES}{3}
% Diese Daten müssen einmalig pro Vorlesung angepasst werden:
\newcommand{\COURSE}{Einführung in Maschinelles Lernen}
\newcommand{\TUTOR}{TBD}
\newcommand{\STUDENTA}{Maria Heitmeier}
\newcommand{\STUDENTB}{Gwent Krause}
\newcommand{\STUDENTC}{Stefan Wezel}
\newcommand{\DEADLINE}{07.06.2018}




%Math
\usepackage{amsmath,amssymb,tabularx}

%Figures
\usepackage{graphicx,tikz,color,float}
\graphicspath{ {home/stefan/picures/} }
\usetikzlibrary{shapes,trees}

%Algorithms
\usepackage[ruled,linesnumbered]{algorithm2e}

%Kopf- und Fußzeile
\pagestyle {fancy}
\fancyhead[L]{\COURSE}
\fancyhead[C]{\STUDENTA, \STUDENTB, \STUDENTC\\}
\fancyhead[R]{\today}

\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Seite \thepage}

%Formatierung der Überschrift, hier nichts ändern
\def\header#1#2{
	\begin{center}
		{\Large\bf Übungsblatt #1}\\
		{(Abgabetermin #2)}
	\end{center}
}

%Definition der Punktetabelle, hier nichts ändern
\newcounter{punktelistectr}
\newcounter{punkte}
\newcommand{\punkteliste}[2]{%
	\setcounter{punkte}{#2}%
	\addtocounter{punkte}{-#1}%
	\stepcounter{punkte}%<-- also punkte = m-n+1 = Anzahl Spalten[1]
	\begin{center}%
		\begin{tabularx}{\linewidth}[]{@{}*{\thepunkte}{>{\centering\arraybackslash} X|}@{}>{\centering\arraybackslash}X}
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				\thepunktelistectr &
			}
			#2 &  $\Sigma$ \\
			\hline
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				&	
			} &\\
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				&
			} &\\
		\end{tabularx}
	\end{center}
}

\begin{document}
\section*{Aufgabe 1}
\subsection*{(a)}
Es ist möglich den Logarithmus zu verwenden, da es sich beim Logarithmus um eine monoton steigende stetige Funktion handelt. Das bedeutet, genau dort, wo das Maximum der Likelihood Funktion ist, ist auch das der Log-Likelihood.\\
Wenn wir zur Log-Likelihood umformen, wird aus dem Produkt eine Summe (Aufgrund von Logarithmus Rechenregeln). Dies ist praktisch, da wir um das Maximum zu finden, ableiten müssen und eine Summe ableiten mehr Spaß macht als ein Produkt, da wir hier auf die Produktregel verzichten können und jeden Term einzeln ableiten können.


\subsection*{(b)}
Für $\mu$ erhält man den Durchschnitt der Sample Daten und unsere beste Schätzung für den Mittelwert der Verteilung. Es handelt sich dabei um einen konkreten Wert.\\
$\sigma$ beschreibt die Varianz der Samples und dementsprechend unsere beste Schätzung für die Varianz der Verteilung. Auch hier ergibt sich ein konkreter Wert.



\subsection*{(c)}
Für die Bayes Parameterschätzung nehmen wir an, dass die gesuchte Funktion, eine durch Parameter beschreibbare ist.\\
Außerdem nehmen wir an, dass sie nur von den Daten der jeweiligen Klasse abhängt.
\\

\subsection*{(d)}

\subsubsection*{Bayes Fehler:}
beschreibt die minimal mögliche Fehlerrate für einen Klassifikator.
\subsubsection*{Modell Fehler:}
Fehler der entsteht durch falsche a priori Wahrscheinlichkeiten.
\subsubsection*{Schätz Fehler:}
Verzerrung des Modells, falls zu wenige Trainingsdaten vorhanden sind.



\section*{Aufgabe 2}
z.z.: Für $x_1,...,x_n$ gezogen aus einer Normalverteilungen $X_1,...,X_n$ mit $\mathcal{N}(\mu, \sigma)$ gilt mit der Maximum Likelihood Methode: $\mu_{MLE} = \frac{\sum_{i=1}^{n} x_i}{n}$\\
\underline{Beweis:}\\
Wir wissen, dass gilt $P(X_1 = x_1 \wedge ... X_n = x_n) = \prod_{i=1}^n P(X_i = x_i)$. Wir müssen das $\mu$ finden, an dem der Ausdruck maximal wird. Aus 1a) wissen wir, dass wir beide Seiten dieser Gleichung logarithmieren dürfen, ohne dass sich das Maximum des Ausdrucks ändert. Diese Eigenschaft nutzen wir im folgenden aus. Der Einfachheit halber definieren wir $P(X_1 = x_1 \wedge ... X_n = x_n) = P$.
\begin{align*}
	P &= \prod_{i=1}^n P(X_i = x_i)\\
	\Leftrightarrow P &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp{(- \frac{(x_i-\mu)^2}{2\sigma^2})}\\
	\Leftrightarrow \log(P) &= \log(\prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp{(- \frac{(x_i-\mu)^2}{2\sigma^2})})\\
	\Leftrightarrow \log(P) &= \sum_{i=1}^{n}\log( \frac{1}{\sqrt{2\pi \sigma^2}} \exp{(- \frac{(x_i-\mu)^2}{2\sigma^2})})\\
	\Leftrightarrow \log(P) &= n \log\frac{1}{\sqrt{2\pi \sigma^2}} + \sum_{i=1}^{n} \log\exp{(- \frac{(x_i-\mu)^2}{2\sigma^2})}\\
	\Leftrightarrow \log(P) &= n \log\frac{1}{\sqrt{2\pi \sigma^2}} + \sum_{i=1}^{n} (- \frac{(x_i-\mu)^2}{2\sigma^2})\\
	\Leftrightarrow \log(P) &= n \log\frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2
\end{align*}
Um nun das Maximum dieses Ausdrucks zu berechnen, müssen wir ihn zunächst ableiten...
\begin{align*}
	(\log(P))' &= 0 - \frac{1}{2\sigma^2}\sum_{i=1}^{n} 2(x_i-\mu)(-1)\\
	&= \frac{1}{\sigma^2}\sum_{i=1}^{n} (x_i-\mu)\\
	&= \frac{1}{\sigma^2}(- n\mu + \sum_{i=1}^{n} x_i)
\end{align*}
...und dieses dann mit 0 gleichsetzen:
\begin{align*}
	0 &= \frac{1}{\sigma^2}(- n\mu + \sum_{i=1}^{n} x_i)\\
	\Leftrightarrow 0 &=- n\mu + \sum_{i=1}^{n} x_i\\
	\Leftrightarrow 0 &= - n\mu + \sum_{i=1}^{n} x_i\\
	\Leftrightarrow n\mu &= \sum_{i=1}^{n} x_i\\
	\Leftrightarrow \mu &= \frac{\sum_{i=1}^{n} x_i}{n}
\end{align*}
\end{document}