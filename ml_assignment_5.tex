%Template
% !TeX spellcheck = de 
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{geometry,forloop,fancyhdr,fancybox,lastpage}
\usepackage{lscape}
\usepackage{listings}
\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=left,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
% Diese Daten müssen pro Blatt angepasst werden:
\newcommand{\NUMBER}{6}
\newcommand{\EXERCISES}{3}
% Diese Daten müssen einmalig pro Vorlesung angepasst werden:
\newcommand{\COURSE}{Einführung in Maschinelles Lernen}
\newcommand{\TUTOR}{TBD}
\newcommand{\STUDENTA}{Maria Heitmeier}
\newcommand{\STUDENTB}{Gwent Krause}
\newcommand{\STUDENTC}{Stefan Wezel}
\newcommand{\DEADLINE}{07.06.2018}




%Math
\usepackage{amsmath,amssymb,tabularx}

%Figures
\usepackage{graphicx,tikz,color,float}
\graphicspath{ {home/stefan/picures/} }
\usetikzlibrary{shapes,trees}

%Algorithms
\usepackage[ruled,linesnumbered]{algorithm2e}

%Kopf- und Fußzeile
\pagestyle {fancy}
\fancyhead[L]{\COURSE}
\fancyhead[C]{\STUDENTA, \STUDENTB, \STUDENTC\\}
\fancyhead[R]{\today}

\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Seite \thepage}

%Formatierung der Überschrift, hier nichts ändern
\def\header#1#2{
	\begin{center}
		{\Large\bf Übungsblatt #1}\\
		{(Abgabetermin #2)}
	\end{center}
}

%Definition der Punktetabelle, hier nichts ändern
\newcounter{punktelistectr}
\newcounter{punkte}
\newcommand{\punkteliste}[2]{%
	\setcounter{punkte}{#2}%
	\addtocounter{punkte}{-#1}%
	\stepcounter{punkte}%<-- also punkte = m-n+1 = Anzahl Spalten[1]
	\begin{center}%
		\begin{tabularx}{\linewidth}[]{@{}*{\thepunkte}{>{\centering\arraybackslash} X|}@{}>{\centering\arraybackslash}X}
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				\thepunktelistectr &
			}
			#2 &  $\Sigma$ \\
			\hline
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				&	
			} &\\
			\forloop{punktelistectr}{#1}{\value{punktelistectr} < #2 } %
			{%
				&
			} &\\
		\end{tabularx}
	\end{center}
}

\begin{document}
\section*{Aufgabe 1}
\subsection*{(a)}
Knoten in einem Bayes Netz stehen für Zufallsvariablen. Die Kanten stehen für Abhängigkeiten. Sind Knoten miteinander verbunden, beinflusst eine Variable die Andere. Den Knoten sind jeweils Wahrscheinlichkeitswerte zugewiesen.


\subsection*{(b)}
\begin{itemize}
	\item \textbf{Marginalisierung:} Marginalisierung bezeichnet das Vorgehen, die Wahrscheinlichkeit eines Knotenzustandes auszurechnen, wenn man einige der Elternknoten nicht kennt. Man marginalisiert dann über diese Elternknoten, d.h. man berechnet die Wahrscheinlichkeit dafür, dass der Elternknoten einen Wert annimmt und abhängig davon, den Wert eines Kindes. Diese Werte addiert man dann.
	\item \textbf{Konditionierung:} Bei der Konditionierung berechnet man die Wahrscheinlichkeit für einen Knotenzustand, gegeben, dass man den Wert eines anderen Knoten bereits kennt. Das heißt also, dass man die Wahrscheinlichkeit für einen Knotenzustand unter der Bedingung (condition), dass ein anderer Knoten einen bestimmten Wert angenommen hat.
	\item \textbf{Inferenz:} Bei der Inferenz berechnet man die Wahrscheinlichkeit, dass der Elternknoten einen bestimmten Wert annimmt nur aus dem Wissen, welchen Wert ein Kindknoten angenommen hat.
\end{itemize}


\subsection*{(c)}
$P(A,B,C) = P(A) \cdot P(B|A) \cdot P(C|A,B)$\\
\\
Fällt die rote Kante weg, so ergibt sich:\\
$P(A,B,C) = P(A) \cdot P(B|A) \cdot P(C|B)$




\subsection*{(d)}
Die Ordnung eines Hidden Markov Models beschreibt um wie viel Zustände zurückgegangen wird um die Übergangswahrscheinlichkeit zu berechnen. 
%TODO evtl ausführlicher

\subsection*{(e)}
$\Omega$ bezeichnet die Menge aller Zustände eines HMM.
Die Menge $V$ behinhaltet die Visible States eines HMM.

\subsection*{(f)}
Die Matrix A beinhaltet die Übergangswahrscheinlichkeiten $P(\omega_j(t+1)|\omega_i(t))=a_{ij}$.\\
Die Matrix B beinhaltet die Übergangswahrscheinlichkieten in sichtbare Zustände gegeben die aktuellen Zustände.


\subsection*{(g)}
\begin{itemize}
	\item \textbf{Bewertungsproblem} Wie wird der Nachfolgezustand anhand der Übergangswahrscheinlichkeiten bestimmt. %TODO stimmt glaub nicht
	Zur Bewertung kann beispielsweise der Forward Algorithmus genutzt werden.
	
	\item \textbf{Dekodierungsproblem} Wie kann man die wahrscheinlichste Folge von hidden states herausfinden, gegeben die Menge aus sichtbaren Zuständen.
	
	\item \textbf{Lernproblem} %TODO
\end{itemize}


\subsection*{(h)}
Der Forward Algorithmus wird genutzt um das Bewertungsproblem zu lösen. Er kann auch zum Dekodieren verwendet werden.
\\
Mithilfe des Backward Algorithmus' wird das Dekodierungsproblem gelöst.
\subsection*{(i)}
Da der Viterbi Decoder ein Maximum ausgibt und der Logarithmus eine streng monoton steigende Funktion ist, hat er das Maximum an der gleichen Stelle wie die nicht logarithmierten Daten.



\section*{Aufgabe 2}
\subsection*{a)}
\begin{align*}
	&P(a_1, b_1, x_1, c_1, d_2) \\
	= &P(a_1) \cdot P(b_1) \cdot P(x_1|a_1, b_1) \cdot P(c_1|x_1) \cdot P(d_2|x_1)\\
	= &0.25 \cdot 0.6 \cdot 0.5 \cdot 0.6 \cdot 0.7 = 0.0315
\end{align*}

\subsection*{b)}
\begin{align*}
	P(x_1|c_2) \overset{\text{Bayes' rule}}{=} &\frac{P(c_2|x)\cdot P(x_1)}{P(c_2)}\\
	\overset{\text{Hinweis}}{=} &\frac{P(c_2|x)\cdot P(x_1)}{P(c_2|x_1)P(x_1) + P(c_2|x_2)P(x_2)}\\
	= &\frac{0.2\cdot 0.445}{0.2\cdot 0.445 + 0.3\cdot 0.445} \approx 0.3483
\end{align*}

\begin{landscape}
	\small
	\subsection*{c)}
	\begin{align*}
	P(a_3|d_1,x_2) \overset{\text{Bayes' rule}}{=} &\frac{P(d_1 \wedge x_2|a_3)P(a_3)}{P(d_1 \wedge x_2)}\\
	\overset{\text{Pfadregel + Hinweis}}{=} &\frac{P(x_2|a_3)P(d_1|x_2)P(a_3)}{P(d_1 \wedge x_2|a_1)P(a_1) + P(d_1 \wedge x_2|a_2)P(a_2) + P(d_1 \wedge x_2|a_3)P(a_3) + P(d_1 \wedge x_2|a_4)P(a_4)}\\
	\overset{\text{Pfadregel}}{=} &\frac{P(x_2|a_3)P(d_1|x_2)P(a_3)}{(P(x_2|a_1)P(a_1) + P(x_2|a_2)P(a_2) + P(x_2|a_3)P(a_3) + P(x_2|a_4)P(a_4))P(d_1|x_2)}\\
	\overset{\text{Pfadregel}}{=} &\frac{P(x_2|a_3)P(a_3)}{P(x_2|a_1)P(a_1) + P(x_2|a_2)P(a_2) + P(x_2|a_3)P(a_3) + P(x_2|a_4)P(a_4)}\\
	\overset{P(a_1)=P(a_2)=P(a_3)=P(a_4)}{=} &\frac{P(x_2|a_3)}{P(x_2|a_1) + P(x_2|a_2) + P(x_2|a_3) + P(x_2|a_4)}\\
	\overset{\text{Marginalisierung}}{=} &\frac{P(x_2|a_3, b_1)P(b_1) + P(x_2|a_3, b_2)P(b_2)}{(P(x_2|a_1, b1) +  P(x_2|a_2, b_1) + P(x_2|a_3, b_1) + P(x_2|a_4, b_1))P(b_1) + (P(x_2|a_1, b2)+ P(x_2|a_2, b_2)+ P(x_2|a_2, b_2)+ P(x_2|a_3, b_2)+ P(x_2|a_4, b_2))P(b_2)}\\
	= &\frac{0.6\cdot 0.6 + 0.9 \cdot 0.4}{(0.5 + 0.4 + 0.6 + 0.8)\cdot 0.6 + (0.3 + 0.2 + 0.9 + 0.7)\cdot 0.4 } = 0.\overline{324}
	\end{align*}
	
	\normalsize
	
	\subsection*{d)}
	\begin{align*}
		P(c_1|d_2, x_1) \overset{\text{Hinweis}}{=} P(c_1 | x_1) = 0.6
	\end{align*}
\end{landscape}


\subsection*{Aufgabe 3}
siehe code.
\end{document}